import numpy as np
import pandas as pd
import xarray as xr
import hvplot.xarray
import hvplot.pandas
import matplotlib.pyplot as plt
import matplotlib.path as mpath
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from neuralprophet import NeuralProphet, set_log_level
import cdsapi
import cartopy.crs as ccrs
from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
import cartopy.feature as cfeature
import urllib3 
urllib3.disable_warnings()
from pathlib import Path 
from matplotlib.animation import FuncAnimation
import cartopy.feature as cf
from cartopy.feature import NaturalEarthFeature
from scipy.integrate import ode
from IPython.display import HTML
from tempfile import NamedTemporaryFile
from matplotlib import animation


#region Data setup

#import the data

clm_temp = pd.read_csv('/Users/rachelalaynahall/Desktop/Desktop - Joshua’s MacBook Air/NP/.conda/tavg3_3d_asm_Nv_daily_48x24.csv')
clm_aod = pd.read_csv('/Users/rachelalaynahall/Desktop/Desktop - Joshua’s MacBook Air/NP/.conda/TOTEXTTAU_daily_48x24.csv')

#change to datetime

clm_temp['date'] = pd.to_datetime(clm_temp['date'])
clm_aod['date'] = pd.to_datetime(clm_aod['date'])

#rename columns (date to time, and TOTEXTTAU to A)

clm_temp.rename(columns = {'date':'time'}, inplace = True)
clm_aod.rename(columns = {'date':'time'}, inplace = True)
clm_aod.rename(columns = {'TOTEXTTAU':'A'}, inplace = True)

#remove extra observations from AOD data

enddate = pd.datetime(1998,12,10)
clm_aod = clm_aod[clm_aod['time'] <= enddate]

dt = clm_temp.set_index(["time", "lat", "lon"]).to_xarray()
ct_period = dt.sel(time = slice('1986-01-01', '1990-12-31'))

da = clm_aod.set_index(["time", "lat", "lon"]).to_xarray()
ca_period = da.sel(time = slice('1986-01-01', '1990-12-31'))

#convert xarrays to dataframes

ct = dt.to_dataframe()
ca = da.to_dataframe()

#export dataframes

filepath = Path('/Users/rachelalaynahall/Desktop/Desktop - Joshua’s MacBook Air/NP/.conda/ct_nc.csv')  
filepath.parent.mkdir(parents=True, exist_ok=True)  
ct.to_csv(filepath)
 
filepath2 = Path('/Users/rachelalaynahall/Desktop/Desktop - Joshua’s MacBook Air/NP/.conda/ca_nc.csv')  
filepath2.parent.mkdir(parents=True, exist_ok=True)  
ca.to_csv(filepath2)

#re-upload files to make df easier to manage

ct = pd.read_csv('/Users/rachelalaynahall/Desktop/Desktop - Joshua’s MacBook Air/NP/.conda/ct_nc.csv')
ca = pd.read_csv('/Users/rachelalaynahall/Desktop/Desktop - Joshua’s MacBook Air/NP/.conda/ca_nc.csv')

ca.rename(columns = {'TOTEXTTAU':'A'}, inplace = True)

#make latitude and longitude non-negative by adding 90 to lat, and 180 to lon

ct["lat"] = ct["lat"] + 90
ct["lon"] = ct["lon"] + 180

ca["lat"] = ca["lat"] + 90
ca["lon"] = ca["lon"] + 180

#make location string with separator ".."

ct["location"] = ct["lat"].astype(str) + ".." + ct["lon"].astype(str)
ca["location"] = ca["lat"].astype(str) + ".." + ca["lon"].astype(str)

#sort values by location, then by time

ct = ct.sort_values(by = ['location', 'time'], ascending = [True, True])
ca = ca.sort_values(by = ['location', 'time'], ascending = [True, True])

#set index as time

ct = ct.set_index('time')
ca = ca.set_index('time')

#remove lat and lon variables

ct = ct.drop(['lat', 'lon'], axis = 1)
ca = ca.drop(['lat', 'lon'], axis = 1)

#export to R for data processing
  
filepath3 = Path('/Users/rachelalaynahall/Desktop/ct1_nc.csv')  
filepath3.parent.mkdir(parents=True, exist_ok=True)  
ct.to_csv(filepath3)

filepath4 = Path('/Users/rachelalaynahall/Desktop/ca1_nc.csv')  
filepath4.parent.mkdir(parents=True, exist_ok=True)  
ca.to_csv(filepath4)

#endregion

### R CODE BLOCK ###

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(tidyverse)
library(readxl)
library(rgdal) 
library(lattice) 
library(dplyr) 
library(tmap)
library(tmaptools)
library(readxl)
library(knitr)
library(reshape2)
library(ggplot2)
library(viridis)
library(terra)
library(scales)
library(tidyr)
library(forecast)
library(astsa)
library(lubridate)
library(marima)
library(climatrends)

```



```{r}

ct_nc = read.csv("~/Desktop/ct1_nc.csv")
ca_nc = read.csv("~/Desktop/ca1_nc.csv")

split_ct_nc = data.frame(split(ct_nc, ct_nc$location))
split_ca_nc = data.frame(split(ca_nc, ca_nc$location))

```


```{r}
rownames(split_ct_nc)<-unlist(split_ct_nc[,1])
rownames(split_ca_nc)<-unlist(split_ca_nc[,1])

```


```{r}
split_ct_nc = split_ct_nc %>% select(-contains(".location"))
split_ct_nc = split_ct_nc %>% select(-contains(".time"))

split_ca_nc = split_ca_nc %>% select(-contains(".location"))
split_ca_nc = split_ca_nc %>% select(-contains(".time"))

```


```{r}
colnames(split_ca_nc) <- gsub('.A','',colnames(split_ca_nc))
colnames(split_ct_nc) <- gsub('.T','',colnames(split_ct_nc))

```


```{r}

write.csv(split_ca_nc, "~/Desktop/splitca_nc.csv", row.names = TRUE)
write.csv(split_ct_nc, "~/Desktop/splitct_nc.csv", row.names = TRUE)


```

### END R CODE BLOCK ###


#region PCA

#import the data from R

clm_t = pd.read_csv('/Users/rachelalaynahall/Desktop/splitct_nc.csv')
clm_a = pd.read_csv('/Users/rachelalaynahall/Desktop/splitca_nc.csv')

#add name to date column

clm_t.rename( columns={'Unnamed: 0':'Date'}, inplace=True )
clm_a.rename( columns={'Unnamed: 0':'Date'}, inplace=True )

#convert to df

clm_t = pd.DataFrame(clm_t)
clm_a = pd.DataFrame(clm_a)

#remove ".X" from column headings

clm_t.columns = clm_t.columns.str.strip('X.')
clm_a.columns = clm_a.columns.str.strip('X.')

#convert date to datetime

clm_t['Date'] = pd.to_datetime(clm_t['Date'])
clm_a['Date'] = pd.to_datetime(clm_a['Date'])

#set date as index

clm_t = clm_t.set_index('Date')
clm_a = clm_a.set_index('Date')

#create features list
loc_list = list(clm_t)
features = loc_list

#create train and test variables of features

x_ct = clm_t.loc[:, features].values
x_ca = clm_a.loc[:, features].values

#create scaler functions

scaler_ct = StandardScaler()
scaler_ca = StandardScaler()

#fit scalers on data

scaler_ct.fit(x_ct)
scaler_ca.fit(x_ca)

#scale data

x_ct = scaler_ct.transform(x_ct)
x_ca = scaler_ca.transform(x_ca)

#create pca function

pca = PCA(n_components = 5)

#fit pca functions on temperature data

pca.fit(x_ct)

#transform data through pca

principalComponents_ct = pca.transform(x_ct)
principalComponents_ca = pca.transform(x_ca)

#create new dfs with principal components

principalDf_ct = pd.DataFrame(data = principalComponents_ct
             , columns = ['principal component 1 - Temp', 'principal component 2 - Temp', 'principal component 3 - Temp', 'principal component 4 - Temp','principal component 5 - Temp'])

principalDf_ca = pd.DataFrame(data = principalComponents_ca
             , columns = ['principal component 1 - AOD', 'principal component 2 - AOD', 'principal component 3 - AOD', 'principal component 4 - AOD','principal component 5 - AOD'])


#add back in date column

principalDf_ct['date'] = pd.date_range(start='1/1/1986', periods = len(principalDf_ct), freq='D')
principalDf_ca['date'] = pd.date_range(start='1/1/1986', periods = len(principalDf_ca), freq='D')

#view explained variance

pca.explained_variance_ratio_

#view dfs

principalDf_ct
principalDf_ca

#separate train and test data

principalDf_ct['date'] = pd.to_datetime(principalDf_ct['date'])
principalDf_ca['date'] = pd.to_datetime(principalDf_ca['date'])

split_date = pd.datetime(1990,12,31)

ct_train = principalDf_ct.loc[principalDf_ct['date'] <= split_date]
ct_test = principalDf_ct.loc[principalDf_ct['date'] > split_date]

ca_train = principalDf_ca.loc[principalDf_ca['date'] <= split_date]
ca_test = principalDf_ca.loc[principalDf_ca['date'] > split_date]

#endregion



#region Create df for each PC

#testing, training, and total sets

df1_test = ct_test['date'].copy()
temp1 = ct_test['principal component 1 - Temp'].copy()
aod1 = ca_test['principal component 1 - AOD'].copy()
df1_test = pd.concat([df1_test, temp1], axis = 1)
df1_test = pd.concat([df1_test, aod1], axis = 1)

df2_test = ct_test['date'].copy()
temp2 = ct_test[["principal component 2 - Temp"]].copy()
aod2 = ca_test[["principal component 2 - AOD"]].copy()
df2_test = pd.concat([df2_test, temp2], axis = 1)
df2_test = pd.concat([df2_test, aod2], axis = 1)

df3_test = ct_test['date'].copy()
temp3 = ct_test[["principal component 3 - Temp"]].copy()
aod3 = ca_test[["principal component 3 - AOD"]].copy()
df3_test = pd.concat([df3_test, temp3], axis = 1)
df3_test = pd.concat([df3_test, aod3], axis = 1)

df4_test = ct_test['date'].copy()
temp4 = ct_test[["principal component 4 - Temp"]] 
aod4 = ca_test[["principal component 4 - AOD"]].copy()
df4_test = pd.concat([df4_test, temp4], axis = 1)
df4_test = pd.concat([df4_test, aod4], axis = 1)

df5_test = ct_test['date'].copy()
temp5= ct_test[["principal component 5 - Temp"]].copy()
aod5 = ca_test[["principal component 5 - AOD"]].copy()
df5_test = pd.concat([df5_test, temp5], axis = 1)
df5_test = pd.concat([df5_test, aod5], axis = 1)

df1_train = ct_train['date'].copy()
temp1a = ct_train['principal component 1 - Temp'].copy()
aod1a = ca_train['principal component 1 - AOD'].copy()
df1_train = pd.concat([df1_train, temp1a], axis = 1)
df1_train = pd.concat([df1_train, aod1a], axis = 1)

df2_train = ct_train['date'].copy()
temp2a = ct_train[["principal component 2 - Temp"]].copy()
aod2a = ca_train[["principal component 2 - AOD"]].copy()
df2_train = pd.concat([df2_train, temp2a], axis = 1)
df2_train = pd.concat([df2_train, aod2a], axis = 1)

df3_train = ct_train['date'].copy()
temp3a = ct_train[["principal component 3 - Temp"]].copy()
aod3a = ca_train[["principal component 3 - AOD"]].copy()
df3_train = pd.concat([df3_train, temp3a], axis = 1)
df3_train = pd.concat([df3_train, aod3a], axis = 1)

df4_train = ct_train['date'].copy()
temp4a = ct_train[["principal component 4 - Temp"]].copy()
aod4a = ca_train[["principal component 4 - AOD"]].copy()
df4_train = pd.concat([df4_train, temp4a], axis = 1)
df4_train = pd.concat([df4_train, aod4a], axis = 1)

df5_train = ct_train['date'].copy()
temp5a = ct_train[["principal component 5 - Temp"]].copy()
aod5a = ca_train[["principal component 5 - AOD"]].copy()
df5_train = pd.concat([df5_train, temp5a], axis = 1)
df5_train = pd.concat([df5_train, aod5a], axis = 1)

df1_train.rename(columns = {'date':'ds'}, inplace = True)
df2_train.rename(columns = {'date':'ds'}, inplace = True)
df3_train.rename(columns = {'date':'ds'}, inplace = True)
df4_train.rename(columns = {'date':'ds'}, inplace = True)
df5_train.rename(columns = {'date':'ds'}, inplace = True)

df1_test.rename(columns = {'date':'ds'}, inplace = True)
df2_test.rename(columns = {'date':'ds'}, inplace = True)
df3_test.rename(columns = {'date':'ds'}, inplace = True)
df4_test.rename(columns = {'date':'ds'}, inplace = True)
df5_test.rename(columns = {'date':'ds'}, inplace = True)

df1_train.rename(columns = {'principal component 1 - Temp':'y'}, inplace = True)
df2_train.rename(columns = {'principal component 2 - Temp':'y'}, inplace = True)
df3_train.rename(columns = {'principal component 3 - Temp':'y'}, inplace = True)
df4_train.rename(columns = {'principal component 4 - Temp':'y'}, inplace = True)
df5_train.rename(columns = {'principal component 5 - Temp':'y'}, inplace = True)

df1_test.rename(columns = {'principal component 1 - Temp':'y'}, inplace = True)
df2_test.rename(columns = {'principal component 2 - Temp':'y'}, inplace = True)
df3_test.rename(columns = {'principal component 3 - Temp':'y'}, inplace = True)
df4_test.rename(columns = {'principal component 4 - Temp':'y'}, inplace = True)
df5_test.rename(columns = {'principal component 5 - Temp':'y'}, inplace = True)

df1_train.rename(columns = {'principal component 1 - AOD':'x'}, inplace = True)
df2_train.rename(columns = {'principal component 2 - AOD':'x'}, inplace = True)
df3_train.rename(columns = {'principal component 3 - AOD':'x'}, inplace = True)
df4_train.rename(columns = {'principal component 4 - AOD':'x'}, inplace = True)
df5_train.rename(columns = {'principal component 5 - AOD':'x'}, inplace = True)

df1_test.rename(columns = {'principal component 1 - AOD':'x'}, inplace = True)
df2_test.rename(columns = {'principal component 2 - AOD':'x'}, inplace = True)
df3_test.rename(columns = {'principal component 3 - AOD':'x'}, inplace = True)
df4_test.rename(columns = {'principal component 4 - AOD':'x'}, inplace = True)
df5_test.rename(columns = {'principal component 5 - AOD':'x'}, inplace = True)


df1 = df1_train
df1 = pd.concat([df1, df1_test], axis = 0)

df2 = df2_train
df2 = pd.concat([df2, df2_test], axis = 0)

df3 = df3_train
df3 = pd.concat([df3, df3_test], axis = 0)

df4 = df4_train
df4 = pd.concat([df4, df4_test], axis = 0)

df5 = df5_train
df5 = pd.concat([df5, df5_test], axis = 0)


#endregion



# region Neural Prophet

set_log_level("ERROR")

# Model and prediction

#PC1

m1 = NeuralProphet(
    growth = 'linear',
    n_lags= 20,
    learning_rate = 0.01,
    seasonality_mode= 'multiplicative',
    yearly_seasonality=True,
    weekly_seasonality=False,
    daily_seasonality=False,
)
m1.set_plotting_backend("matplotlib")

m1.add_lagged_regressor('x')

fig1a = df1.plot(x="ds", y=["y", "x"], figsize=(10, 6))

metrics1 = m1.fit(df1_train, freq = 'D', validation_df = df1_test, progress = None)
metrics1

test_metrics1 = m1.test(df1_test)
test_metrics1

fig, ax = plt.subplots(figsize=(20, 8))
ax.plot(metrics1["MAE"], '-o', label="Training Loss")  
ax.plot(metrics1["MAE_val"], '-r', label="Validation Loss")
ax.legend(loc='center right', fontsize=16)
ax.tick_params(axis='both', which='major', labelsize=20)
ax.set_xlabel("Epoch", fontsize=28)
ax.set_ylabel("Loss", fontsize=28)
ax.set_title("Model Loss (MAE) - PC1", fontsize=28)

forecast1 = m1.predict(df1)
m1.plot(forecast1)

m1.plot_components(forecast1, components=["lagged_regressors"])
m1.plot_parameters(components=["lagged_regressors"])
m1.plot_components(forecast1)
m1.plot_parameters(components=["autoregression"])
m1.plot_components(forecast1, components=["autoregression"])
m1.plot_components(forecast1, components=["seasonality"])
m1.plot_parameters(components=["seasonality"])

df_residuals1 = pd.DataFrame({"ds": df1["ds"], "residuals": df1["y"] - forecast1["yhat1"]})
fig1 = df_residuals1.plot(x="ds", y="residuals", figsize=(10, 6))


#PC2

m2 = NeuralProphet(
    growth = 'linear',
    n_lags= 20,
    learning_rate = 0.01,
    seasonality_mode= 'multiplicative',
    yearly_seasonality=True,
    weekly_seasonality=False,
    daily_seasonality=False,
)
m2.set_plotting_backend("matplotlib")

m2.add_lagged_regressor('x')

fig2a = df2.plot(x="ds", y=["y", "x"], figsize=(10, 6))

metrics2 = m2.fit(df2_train, freq = 'D', validation_df = df2_test, progress = None)
metrics2

test_metrics2 = m2.test(df2_test)
test_metrics2

fig, ax = plt.subplots(figsize=(20, 8))
ax.plot(metrics1["MAE"], '-o', label="Training Loss")  
ax.plot(metrics1["MAE_val"], '-r', label="Validation Loss")
ax.legend(loc='center right', fontsize=16)
ax.tick_params(axis='both', which='major', labelsize=20)
ax.set_xlabel("Epoch", fontsize=28)
ax.set_ylabel("Loss", fontsize=28)
ax.set_title("Model Loss (MAE) - PC2", fontsize=28)

forecast2 = m2.predict(df2)
m2.plot(forecast2)

m2.plot_components(forecast2, components=["lagged_regressors"])
m2.plot_parameters(components=["lagged_regressors"])
m2.plot_components(forecast2)
m2.plot_parameters(components=["autoregression"])
m2.plot_components(forecast2, components=["autoregression"])
m2.plot_components(forecast2, components=["seasonality"])
m2.plot_parameters(components=["seasonality"])

df_residuals2 = pd.DataFrame({"ds": df2["ds"], "residuals": df2["y"] - forecast2["yhat1"]})
fig2 = df_residuals2.plot(x="ds", y="residuals", figsize=(10, 6))



#PC3

m3 = NeuralProphet(
    growth = 'linear',
    n_lags= 20,
    learning_rate = 0.01,
    seasonality_mode= 'multiplicative',
    yearly_seasonality=True,
    weekly_seasonality=False,
    daily_seasonality=False,
)
m3.set_plotting_backend("matplotlib")

m3.add_lagged_regressor('x')

fig3a = df3.plot(x="ds", y=["y", "x"], figsize=(10, 6))

metrics3 = m3.fit(df3_train, freq = 'D', validation_df = df3_test, progress = None)
metrics3

test_metrics3 = m3.test(df3_test)
test_metrics3

fig, ax = plt.subplots(figsize=(20, 8))
ax.plot(metrics3["MAE"], '-o', label="Training Loss")  
ax.plot(metrics3["MAE_val"], '-r', label="Validation Loss")
ax.legend(loc='center right', fontsize=16)
ax.tick_params(axis='both', which='major', labelsize=20)
ax.set_xlabel("Epoch", fontsize=28)
ax.set_ylabel("Loss", fontsize=28)
ax.set_title("Model Loss (MAE) - PC3", fontsize=28)

forecast3 = m3.predict(df3)
m3.plot(forecast3)

m3.plot_components(forecast3, components=["lagged_regressors"])
m3.plot_parameters(components=["lagged_regressors"])
m3.plot_components(forecast3)
m3.plot_parameters(components=["autoregression"])
m3.plot_components(forecast3, components=["autoregression"])
m3.plot_components(forecast3, components=["seasonality"])
m3.plot_parameters(components=["seasonality"])

df_residuals3 = pd.DataFrame({"ds": df3["ds"], "residuals": df3["y"] - forecast3["yhat1"]})
fig3 = df_residuals3.plot(x="ds", y="residuals", figsize=(10, 6))


#PC4

m4 = NeuralProphet(
    growth = 'linear',
    n_lags= 20,
    learning_rate = 0.01,
    seasonality_mode= 'multiplicative',
    yearly_seasonality=True,
    weekly_seasonality=False,
    daily_seasonality=False,
)
m4.set_plotting_backend("matplotlib")

m4.add_lagged_regressor('x')

fig4a = df4.plot(x="ds", y=["y", "x"], figsize=(10, 6))

metrics4 = m4.fit(df4_train, freq = 'D', validation_df = df4_test, progress = None)
metrics4

test_metrics4 = m4.test(df4_test)
test_metrics4

fig, ax = plt.subplots(figsize=(20, 8))
ax.plot(metrics4["MAE"], '-o', label="Training Loss")  
ax.plot(metrics4["MAE_val"], '-r', label="Validation Loss")
ax.legend(loc='center right', fontsize=16)
ax.tick_params(axis='both', which='major', labelsize=20)
ax.set_xlabel("Epoch", fontsize=28)
ax.set_ylabel("Loss", fontsize=28)
ax.set_title("Model Loss (MAE) - PC4", fontsize=28)

forecast4 = m4.predict(df4)
m4.plot(forecast4)

m4.plot_components(forecast4, components=["lagged_regressors"])
m4.plot_parameters(components=["lagged_regressors"])
m4.plot_components(forecast4)
m4.plot_parameters(components=["autoregression"])
m4.plot_components(forecast4, components=["autoregression"])
m4.plot_components(forecast4, components=["seasonality"])
m4.plot_parameters(components=["seasonality"])

df_residuals4 = pd.DataFrame({"ds": df4["ds"], "residuals": df4["y"] - forecast4["yhat1"]})
fig4 = df_residuals4.plot(x="ds", y="residuals", figsize=(10, 6))



#PC5

m5 = NeuralProphet(
    growth = 'linear',
    n_lags= 20,
    learning_rate = 0.01,
    seasonality_mode= 'multiplicative',
    yearly_seasonality=True,
    weekly_seasonality=False,
    daily_seasonality=False,
)
m5.set_plotting_backend("matplotlib")

m5.add_lagged_regressor('x')

fig5a = df5.plot(x="ds", y=["y", "x"], figsize=(10, 6))

metrics5 = m5.fit(df5_train, freq = 'D', validation_df = df5_test, progress = None)
metrics5

test_metrics5 = m5.test(df5_test)
test_metrics5

fig, ax = plt.subplots(figsize=(20, 8))
ax.plot(metrics5["MAE"], '-o', label="Training Loss")  
ax.plot(metrics5["MAE_val"], '-r', label="Validation Loss")
ax.legend(loc='center right', fontsize=16)
ax.tick_params(axis='both', which='major', labelsize=20)
ax.set_xlabel("Epoch", fontsize=28)
ax.set_ylabel("Loss", fontsize=28)
ax.set_title("Model Loss (MAE) - PC5", fontsize=28)

forecast5 = m5.predict(df5)
m5.plot(forecast5)

m5.plot_components(forecast5, components=["lagged_regressors"])
m5.plot_parameters(components=["lagged_regressors"])
m5.plot_components(forecast5)
m5.plot_parameters(components=["autoregression"])
m5.plot_components(forecast5, components=["autoregression"])
m5.plot_components(forecast5, components=["seasonality"])
m5.plot_parameters(components=["seasonality"])

df_residuals5 = pd.DataFrame({"ds": df5["ds"], "residuals": df5["y"] - forecast5["yhat1"]})
fig5 = df_residuals5.plot(x="ds", y="residuals", figsize=(10, 6))



#display(summary(m.model))


#endregion




# region Backscale


yhat1 = forecast1.loc[forecast1['ds'] > split_date]
df1_predict = df1_train['y'].copy()
yhat1 = yhat1['yhat1'].copy()
df1_predict = pd.concat([df1_predict, yhat1], axis = 0)

yhat2 = forecast2.loc[forecast2['ds'] > split_date]
df2_predict = df2_train['y'].copy()
yhat2 = yhat2['yhat1'].copy()
df2_predict = pd.concat([df2_predict, yhat2], axis = 0)

yhat3 = forecast3.loc[forecast3['ds'] > split_date]
df3_predict = df3_train['y'].copy()
yhat3 = yhat3['yhat1'].copy()
df3_predict = pd.concat([df3_predict, yhat3], axis = 0) 

yhat4 = forecast4.loc[forecast4['ds'] > split_date]
df4_predict = df4_train['y'].copy()
yhat4 = yhat4['yhat1'].copy()
df4_predict = pd.concat([df4_predict, yhat4], axis = 0)

yhat5 = forecast5.loc[forecast5['ds'] > split_date]
df5_predict = df5_train['y'].copy()
yhat5 = yhat5['yhat1'].copy()
df5_predict = pd.concat([df5_predict, yhat5], axis = 0)

df_predict = pd.concat([df1_predict, df2_predict, df3_predict, df4_predict, df5_predict], axis = 1)

ct_orig = np.dot(df_predict,pca.components_)
ct_orig_backscaled = scaler_ct.inverse_transform(ct_orig)

ct_orig = pd.DataFrame(ct_orig)
ct_orig_backscaled = pd.DataFrame(ct_orig_backscaled)

ct_orig_backscaled['date'] = pd.date_range(start='1/1/1986', periods = len(principalDf_ct), freq='D')

ct_orig_backscaled = ct_orig_backscaled.set_index('date')
ct_orig_backscaled.columns = clm_t.columns

df_ca = principalDf_ca
df_ca = df_ca.drop(['date'], axis = 1)

ca_orig = np.dot(df_ca,pca.components_)
ca_orig_backscaled = scaler_ca.inverse_transform(ca_orig)

ca_orig = pd.DataFrame(ca_orig)
ca_orig_backscaled = pd.DataFrame(ca_orig_backscaled)

ca_orig_backscaled['date'] = pd.date_range(start='1/1/1986', periods = len(principalDf_ca), freq='D')

ca_orig_backscaled = ca_orig_backscaled.set_index('date')
ca_orig_backscaled.columns = clm_a.columns

#non-predicted values back-scaled

df_ct = principalDf_ct
df_ct = df_ct.drop(['date'], axis = 1)

ct_np = np.dot(df_ct,pca.components_)
ct_np_backscaled = scaler_ct.inverse_transform(ct_np)

ct_np = pd.DataFrame(ct_np)
ct_np_backscaled = pd.DataFrame(ct_np_backscaled)

ct_np_backscaled['date'] = pd.date_range(start='1/1/1986', periods = len(principalDf_ca), freq='D')

ct_np_backscaled = ct_np_backscaled.set_index('date')
ct_np_backscaled.columns = clm_t.columns

#export to R for data processing

filepath5 = Path('/Users/rachelalaynahall/Desktop/ct_bs_nc.csv')  
filepath5.parent.mkdir(parents=True, exist_ok=True)  
ct_orig_backscaled.to_csv(filepath5)

filepath6 = Path('/Users/rachelalaynahall/Desktop/ca_bs_nc.csv')  
filepath6.parent.mkdir(parents=True, exist_ok=True)  
ca_orig_backscaled.to_csv(filepath6)

filepath7 = Path('/Users/rachelalaynahall/Desktop/ct_np_nc.csv')  
filepath7.parent.mkdir(parents=True, exist_ok=True)  
ct_np_backscaled.to_csv(filepath7)


### R CODE BLOCK ###


```{r}
ct_bs_nc = read.csv("~/Desktop/ct_bs_nc.csv")

```


```{r}
rownames(ct_bs_nc)<-unlist(ct_bs_nc[,1])
ct_bs_nc = ct_bs_nc[-c(1)]

ct_bs_nc = ct_bs_nc %>% rownames_to_column('row') %>% pivot_longer(cols = -row)

ct_bs_nc = ct_bs_nc %>% 
  rename(
    date = row,
    location = name,
    T = value
    )
```


```{r}
write.csv(ct_bs_nc, "~/Desktop/nc_ct_bs.csv", row.names = FALSE)
```


```{r}
ca_bs_nc = read.csv("~/Desktop/ca_bs_nc.csv")

```


```{r}
rownames(ca_bs_nc)<-unlist(ca_bs_nc[,1])
ca_bs_nc = ca_bs_nc[-c(1)]

ca_bs_nc = ca_bs_nc %>% rownames_to_column('row') %>% pivot_longer(cols = -row)

ca_bs_nc = ca_bs_nc %>% 
  rename(
    date = row,
    location = name,
    A = value
    )
```


```{r}
write.csv(ca_bs_nc, "~/Desktop/nc_ca_bs.csv", row.names = FALSE)
```


```{r}
ct_np_nc = read.csv("~/Desktop/ct_np_nc.csv")

```


```{r}
rownames(ct_np_nc)<-unlist(ct_np_nc[,1])
ct_np_nc = ct_np_nc[-c(1)]

ct_np_nc = ct_np_nc %>% rownames_to_column('row') %>% pivot_longer(cols = -row)

ct_np_nc = ct_np_nc %>% 
  rename(
    date = row,
    location = name,
    T = value
    )
```


```{r}
write.csv(ct_np_nc, "~/Desktop/nc_ct_np.csv", row.names = FALSE)
```


### END R CODE BLOCK ###


#import from R

ct_bs = pd.read_csv('/Users/rachelalaynahall/Desktop/nc_ct_bs.csv')

ct_bs[['lat', 'lon']] = ct_bs["location"].apply(lambda x: pd.Series(str(x).split("..")))

ct_bs['lat'] = ct_bs['lat'].str.replace('X', '')
ct_bs["lat"] = ct_bs["lat"].astype(float)
ct_bs["lon"] = ct_bs["lon"].astype(float)
ct_bs["lat"] = ct_bs["lat"] - 90
ct_bs["lon"] = ct_bs["lon"] - 180
ct_bs = ct_bs.drop(['location'], axis = 1)
ct_bs['date'] = pd.to_datetime(ct_bs['date'])
ct_bs.rename(columns = {'date':'time'}, inplace = True)

ct_bs_array = ct_bs.set_index(["time", "lat", "lon"]).to_xarray()

tf = ct_bs_array.to_dataframe()

compare = dt.to_dataframe()

ca_bs = pd.read_csv('/Users/rachelalaynahall/Desktop/nc_ca_bs.csv')

ca_bs[['lat', 'lon']] = ca_bs["location"].apply(lambda x: pd.Series(str(x).split("..")))

ca_bs['lat'] = ca_bs['lat'].str.replace('X', '')
ca_bs["lat"] = ca_bs["lat"].astype(float)
ca_bs["lon"] = ca_bs["lon"].astype(float)
ca_bs["lat"] = ca_bs["lat"] - 90
ca_bs["lon"] = ca_bs["lon"] - 180
ca_bs = ca_bs.drop(['location'], axis = 1)
ca_bs['date'] = pd.to_datetime(ca_bs['date'])
ca_bs.rename(columns = {'date':'time'}, inplace = True)

ca_bs_array = ca_bs.set_index(["time", "lat", "lon"]).to_xarray()

af = ca_bs_array.to_dataframe()

compare_a = da.to_dataframe()

# non-predicted temperature

ct_np = pd.read_csv('/Users/rachelalaynahall/Desktop/nc_ct_np.csv')

ct_np[['lat', 'lon']] = ct_np["location"].apply(lambda x: pd.Series(str(x).split("..")))

ct_np['lat'] = ct_np['lat'].str.replace('X', '')
ct_np["lat"] = ct_np["lat"].astype(float)
ct_np["lon"] = ct_np["lon"].astype(float)
ct_np["lat"] = ct_np["lat"] - 90
ct_np["lon"] = ct_np["lon"] - 180
ct_np = ct_np.drop(['location'], axis = 1)
ct_np['date'] = pd.to_datetime(ct_np['date'])
ct_np.rename(columns = {'date':'time'}, inplace = True)

ct_np_array = ct_np.set_index(["time", "lat", "lon"]).to_xarray()

tn = ct_np_array.to_dataframe()

compare_t = da.to_dataframe()

#export to R for data processing

filepath8 = Path('/Users/rachelalaynahall/Desktop/tf_nc.csv')  
filepath8.parent.mkdir(parents=True, exist_ok=True)  
tf.to_csv(filepath8)

filepath9 = Path('/Users/rachelalaynahall/Desktop/compare_nc.csv')  
filepath9.parent.mkdir(parents=True, exist_ok=True)  
compare.to_csv(filepath9)

filepath10 = Path('/Users/rachelalaynahall/Desktop/af_nc.csv')  
filepath10.parent.mkdir(parents=True, exist_ok=True)  
af.to_csv(filepath10)

filepath11 = Path('/Users/rachelalaynahall/Desktop/compare_a_nc.csv')  
filepath11.parent.mkdir(parents=True, exist_ok=True)  
compare_a.to_csv(filepath11)

filepath12 = Path('/Users/rachelalaynahall/Desktop/tn_nc.csv')  
filepath12.parent.mkdir(parents=True, exist_ok=True)  
tn.to_csv(filepath12)

filepath13 = Path('/Users/rachelalaynahall/Desktop/compare_t_nc.csv')  
filepath13.parent.mkdir(parents=True, exist_ok=True)  
compare_t.to_csv(filepath13)

#endregion


### R CODE BLOCK ###



```{r}
tf_nc = read.csv("~/Desktop/tf_nc.csv")
compare_nc = read.csv("~/Desktop/compare_nc.csv")

difference_nc = tf_nc['time']
difference_nc['lat'] = tf_nc['lat']
difference_nc['lon'] = tf_nc['lon']
difference_nc['diff'] = compare_nc['T'] - tf_nc['T']

difference_nc$location = str_c(difference_nc$lat, ', ', difference_nc$lon)

difference_nc['diff2'] = difference_nc['diff']^2

af_nc = read.csv("~/Desktop/af_nc.csv")
compare_a_nc = read.csv("~/Desktop/compare_a_nc.csv")

difference_a_nc = af_nc['time']
difference_a_nc['lat'] = af_nc['lat']
difference_a_nc['lon'] = af_nc['lon']
difference_a_nc['diff'] = compare_a_nc['A'] - af_nc['A']

difference_a_nc$location = str_c(difference_a_nc$lat, ', ', difference_a_nc$lon)

difference_a_nc['diff2'] = difference_a_nc['diff']^2

tn_nc = read.csv("~/Desktop/tn_nc.csv")

difference_t_nc = tn_nc['time']
difference_t_nc['lat'] = tn_nc['lat']
difference_t_nc['lon'] = tn_nc['lon']
difference_t_nc['diff'] = compare_nc['T'] - tn_nc['T']

difference_t_nc$location = str_c(difference_t_nc$lat, ', ', difference_t_nc$lon)

difference_t_nc['diff2'] = difference_t_nc['diff']^2

```


```{r}
write.csv(difference_nc, "~/Desktop/nc_difference.csv", row.names = FALSE)

write.csv(difference_a_nc, "~/Desktop/nc_difference_a.csv", row.names = FALSE)

write.csv(difference_t_nc, "~/Desktop/nc_difference_t.csv", row.names = FALSE)
```


```{r}

diff_loc_nc = difference_nc %>%
  group_by(lat,lon) %>%
  summarise(mean_diff = mean(diff))

min_nc = min(diff_loc_nc$mean_diff)
min_nc

max_nc = max(diff_loc_nc$mean_diff)
max_nc

range_nc = max_nc - min_nc
range_nc

mean_locnc = mean(diff_loc_nc$mean_diff)
mean_locnc

diff_loc_nc$mean_diff = diff_loc_nc$mean_diff^2
diff_loc_nc$mean_diff = rescale(diff_loc_nc$mean_diff, to = c(0, 10))

ggplot(diff_loc_nc, aes(x = lon, y = lat)) +
  geom_tile(aes(fill = mean_diff), colour = "white") +
  scale_fill_gradient(low = "white", high = "black") +
  theme_minimal() + 
  ggtitle("Temperature Mean Diff^2 by Location (No Climatology, Predicted Data)")

diff_loc_a_nc = difference_a_nc %>%
  group_by(lat,lon) %>%
  summarise(mean_diff = mean(diff))

min_anc = min(diff_loc_a_nc$mean_diff)
min_anc

max_anc = max(diff_loc_a_nc$mean_diff)
max_anc

diff_loc_a_nc$mean_diff = diff_loc_a_nc$mean_diff^2
diff_loc_a_nc$mean_diff = rescale(diff_loc_a_nc$mean_diff, to = c(0, 10))

ggplot(diff_loc_a_nc, aes(x = lon, y = lat)) +
  geom_tile(aes(fill = mean_diff), colour = "white") +
  scale_fill_gradient(low = "white", high = "black") +
  theme_minimal() + 
  ggtitle("AOD Mean Diff^2 by Location (No Climatology)")

diff_loc_t_nc = difference_t_nc %>%
  group_by(lat,lon) %>%
  summarise(mean_diff = mean(diff))

min_tnc = min(diff_loc_t_nc$mean_diff)
min_tnc

max_tnc = max(diff_loc_t_nc$mean_diff)
max_tnc

diff_loc_t_nc$mean_diff = diff_loc_t_nc$mean_diff^2
diff_loc_t_nc$mean_diff = rescale(diff_loc_t_nc$mean_diff, to = c(0, 10))

ggplot(diff_loc_t_nc, aes(x = lon, y = lat)) +
  geom_tile(aes(fill = mean_diff), colour = "white") +
  scale_fill_gradient(low = "white", high = "black") +
  theme_minimal() + 
  ggtitle("Temperature Mean Diff^2 by Location (No Climatology)")

```


```{r}
ggplot(data=difference_nc %>% mutate(time=as_date(time)) %>% group_by(time) %>% summarize(m=mean(diff^2)),aes(x=time,y=m)) + geom_line() + 
  ggtitle("Temperature Mean Diff^2 by Date (No Climatology, Predicted Data)")

ggplot(data=difference_a_nc %>% mutate(time=as_date(time)) %>% group_by(time) %>% summarize(m=mean(diff^2)),aes(x=time,y=m)) + geom_line() + 
  ggtitle("AOD Mean Diff^2 by Date (No Climatology)")

ggplot(data=difference_t_nc %>% mutate(time=as_date(time)) %>% group_by(time) %>% summarize(m=mean(diff^2)),aes(x=time,y=m)) + geom_line() + 
  ggtitle("Temperature Mean Diff^2 by Date (No Climatology)")

diff_time_nc = difference_nc %>%
  group_by(time) %>%
  summarise(mean_diff = mean(diff))

min_time_nc = min(diff_time_nc$mean_diff)
min_time_nc

max_time_nc = max(diff_time_nc$mean_diff)
max_time_nc

range_time_nc = max_time_nc - min_time_nc
range_time_nc

mean_timenc = mean(diff_time_nc$mean_diff)
mean_timenc

```

### END R CODE BLOCK ###



#region Plots

temperature = ct_bs_array['T']
aod = ca_bs_array['A']

ct_bs_array.hvplot(groupby = 'time', cmap = "fire")

ct_bs_array.hvplot(
    groupby="time",  # adds a widget for time
    clim=(-15, 15),  # sets colormap limits
    widget_type="scrubber",
    widget_location="bottom",
    cmap = "seismic"
)

ca_bs_array.hvplot(
    groupby="time",  # adds a widget for time
    clim=(-0.1, .4),  # sets colormap limits
    widget_type="scrubber",
    widget_location="bottom",
    cmap = "BuPu"
)



nbr = 4727


for j  in range(nbr):
    cbar_kwargs = {
        'orientation':'horizontal',
        'fraction': 0.045,
        'pad': 0.01,
        'extend':'neither'
    }

    fig = plt.figure(figsize=(20,20))
    ax = fig.add_subplot(1,1,1, projection = ccrs.PlateCarree())
    ax.add_feature(NaturalEarthFeature('cultural', 'admin_0_countries', '10m'),
                        facecolor='none', edgecolor='black')
    ax.set_extent([-150, 150, -55, 85])

    date =  pd.to_datetime(temperature.isel(time=j )['time'].values)
    ax.set_title("Temperature on " + str(date.month) + "/" + str(date.day) + "/" + str(date.year))
    temperature.isel(time=j ).plot.imshow(ax=ax, add_labels=False, add_colorbar=True,
                vmin=241.5, vmax=263.5, cmap='coolwarm',
                cbar_kwargs=cbar_kwargs, interpolation='bicubic')
    plt.savefig("5_temp_global_map" + str(j ) + ".png", bbox_inches='tight', dpi=150)


for j  in range(nbr):
    cbar_kwargs = {
        'orientation':'horizontal',
        'fraction': 0.045,
        'pad': 0.01,
        'extend':'neither'
    }

    fig = plt.figure(figsize=(20,20))
    ax = fig.add_subplot(1,1,1, projection = ccrs.PlateCarree())
    ax.add_feature(NaturalEarthFeature('cultural', 'admin_0_countries', '10m'),
                        facecolor='none', edgecolor='black')
    ax.set_extent([-150, 150, -55, 85])

    date =  pd.to_datetime(aod.isel(time=j )['time'].values)
    ax.set_title("AOD on " + str(date.month) + "/" + str(date.day) + "/" + str(date.year))
    aod.isel(time=j ).plot.imshow(ax=ax, add_labels=False, add_colorbar=True,
                vmin=-0.1, vmax=0.4, cmap='BuPu',
                cbar_kwargs=cbar_kwargs, interpolation='bicubic')
    plt.savefig("5_aod_global_map" + str(j ) + ".png", bbox_inches='tight', dpi=150)




#endregion

n_pcs= pca.n_components_ # get number of component
# get the index of the most important feature on EACH component
most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]
initial_feature_names = clm_t.columns
# get the most important feature names
most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]
most_important_names





