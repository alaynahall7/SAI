import numpy as np
import pandas as pd
import xarray as xr
import hvplot.xarray
import hvplot.pandas
import matplotlib.pyplot as plt
import matplotlib.path as mpath
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from neuralprophet import NeuralProphet, set_log_level
import cdsapi
import cartopy.crs as ccrs
from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
import cartopy.feature as cfeature
import urllib3 
urllib3.disable_warnings()
from pathlib import Path 
from matplotlib.animation import FuncAnimation
import cartopy.feature as cf
from cartopy.feature import NaturalEarthFeature
from scipy.integrate import ode
from IPython.display import HTML
from tempfile import NamedTemporaryFile
from matplotlib import animation
import seaborn as sns



#region Data setup

#import the data

clm_temp = pd.read_csv('/Users/rachelalaynahall/Desktop/Desktop - Joshua’s MacBook Air/NP/.conda/tavg3_3d_asm_Nv_daily_48x24.csv')
clm_aod = pd.read_csv('/Users/rachelalaynahall/Desktop/Desktop - Joshua’s MacBook Air/NP/.conda/TOTEXTTAU_daily_48x24.csv')

#change to datetime

clm_temp['date'] = pd.to_datetime(clm_temp['date'])
clm_aod['date'] = pd.to_datetime(clm_aod['date'])

#rename columns (date to time, and TOTEXTTAU to A)

clm_temp.rename(columns = {'date':'time'}, inplace = True)
clm_aod.rename(columns = {'date':'time'}, inplace = True)
clm_aod.rename(columns = {'TOTEXTTAU':'A'}, inplace = True)

#remove extra observations from AOD data

enddate = pd.datetime(1998,12,10)
clm_aod = clm_aod[clm_aod['time'] <= enddate]

#subtract mean (climatology) for each month from 01/01/1986 through 12/31/1990

dt = clm_temp.set_index(["time", "lat", "lon"]).to_xarray()
ct_period = dt.sel(time = slice('1986-01-01', '1990-12-31'))
ct_month = ct_period.groupby('time.month').mean()
at_month = dt.groupby('time.month') - ct_month

da = clm_aod.set_index(["time", "lat", "lon"]).to_xarray()
ca_period = da.sel(time = slice('1986-01-01', '1990-12-31'))
ca_month = ca_period.groupby('time.month').mean()
aa_month = da.groupby('time.month') - ca_month

#convert xarrays to dataframes

ct = at_month.to_dataframe()
ca = aa_month.to_dataframe()

#export dataframes

filepath = Path('/Users/rachelalaynahall/Desktop/Desktop - Joshua’s MacBook Air/NP/.conda/ct.csv')  
filepath.parent.mkdir(parents=True, exist_ok=True)  
ct.to_csv(filepath)
 
filepath2 = Path('/Users/rachelalaynahall/Desktop/Desktop - Joshua’s MacBook Air/NP/.conda/ca.csv')  
filepath2.parent.mkdir(parents=True, exist_ok=True)  
ca.to_csv(filepath2)

#re-upload files to make df easier to manage

ct = pd.read_csv('/Users/rachelalaynahall/Desktop/Desktop - Joshua’s MacBook Air/NP/.conda/ct.csv')
ca = pd.read_csv('/Users/rachelalaynahall/Desktop/Desktop - Joshua’s MacBook Air/NP/.conda/ca.csv')

ca.rename(columns = {'TOTEXTTAU':'A'}, inplace = True)

#make latitude and longitude non-negative by adding 90 to lat, and 180 to lon

ct["lat"] = ct["lat"] + 90
ct["lon"] = ct["lon"] + 180

ca["lat"] = ca["lat"] + 90
ca["lon"] = ca["lon"] + 180

#make location string with separator ".."

ct["location"] = ct["lat"].astype(str) + ".." + ct["lon"].astype(str)
ca["location"] = ca["lat"].astype(str) + ".." + ca["lon"].astype(str)

#sort values by location, then by time

ct = ct.sort_values(by = ['location', 'time'], ascending = [True, True])
ca = ca.sort_values(by = ['location', 'time'], ascending = [True, True])

#set index as time

ct = ct.set_index('time')
ca = ca.set_index('time')

#remove lat, lon, and month variables

ct = ct.drop(['lat', 'lon', 'month'], axis = 1)
ca = ca.drop(['lat', 'lon', 'month'], axis = 1)

#export to R for data processing
  
filepath3 = Path('/Users/rachelalaynahall/Desktop/ct1.csv')  
filepath3.parent.mkdir(parents=True, exist_ok=True)  
ct.to_csv(filepath3)

filepath4 = Path('/Users/rachelalaynahall/Desktop/ca1.csv')  
filepath4.parent.mkdir(parents=True, exist_ok=True)  
ca.to_csv(filepath4)

#endregion

### R CODE BLOCK ###


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(tidyverse)
library(readxl)
library(rgdal) 
library(lattice) 
library(dplyr) 
library(tmap)
library(tmaptools)
library(readxl)
library(knitr)
library(reshape2)
library(ggplot2)
library(viridis)
library(terra)
library(scales)
library(tidyr)
library(forecast)
library(astsa)
library(lubridate)
library(marima)
library(climatrends)

```



```{r}

ct = read.csv("~/Desktop/ct1.csv")
ca = read.csv("~/Desktop/ca1.csv")


split_ct = data.frame(split(ct, ct$location))
split_ca = data.frame(split(ca, ca$location))

```


```{r}
rownames(split_ct)<-unlist(split_ct[,1])
rownames(split_ca)<-unlist(split_ca[,1])

```


```{r}
split_ct = split_ct %>% select(-contains(".location"))
split_ct = split_ct %>% select(-contains(".time"))

split_ca = split_ca %>% select(-contains(".location"))
split_ca = split_ca %>% select(-contains(".time"))
```


```{r}

colnames(split_ca) <- gsub('.A','',colnames(split_ca))
colnames(split_ct) <- gsub('.T','',colnames(split_ct))

```


```{r}

write.csv(split_ca, "~/Desktop/splitca.csv", row.names = TRUE)
write.csv(split_ct, "~/Desktop/splitct.csv", row.names = TRUE)


```
### END R CODE BLOCK ###


#region PCA

#import the data from R

clm_t = pd.read_csv('/Users/rachelalaynahall/Desktop/splitct.csv')
clm_a = pd.read_csv('/Users/rachelalaynahall/Desktop/splitca.csv')

#add name to date column

clm_t.rename( columns={'Unnamed: 0':'Date'}, inplace=True )
clm_a.rename( columns={'Unnamed: 0':'Date'}, inplace=True )

#convert to df

clm_t = pd.DataFrame(clm_t)
clm_a = pd.DataFrame(clm_a)

#remove ".X" from column headings

clm_t.columns = clm_t.columns.str.strip('X.')
clm_a.columns = clm_a.columns.str.strip('X.')

#convert date to datetime

clm_t['Date'] = pd.to_datetime(clm_t['Date'])
clm_a['Date'] = pd.to_datetime(clm_a['Date'])

#set date as index

clm_t = clm_t.set_index('Date')
clm_a = clm_a.set_index('Date')

#create features list

loc_list = list(clm_t)
features = loc_list

x_ct = clm_t.loc[:, features].values
x_ca = clm_a.loc[:, features].values

#create scaler functions

scaler_ct = StandardScaler()
scaler_ca = StandardScaler()

#fit scalers on data

scaler_ct.fit(x_ct)
scaler_ca.fit(x_ca)

#scale data

x_ct = scaler_ct.transform(x_ct)
x_ca = scaler_ca.transform(x_ca)

#create pca function

pca = PCA(n_components = 5)

#fit pca function on temperature data

pca.fit(x_ct)

#transform data through pca

principalComponents_ct = pca.transform(x_ct)
principalComponents_ca = pca.transform(x_ca)

#create new dfs with principal components

principalDf_ct = pd.DataFrame(data = principalComponents_ct
             , columns = ['principal component 1 - Temp', 'principal component 2 - Temp', 'principal component 3 - Temp', 'principal component 4 - Temp','principal component 5 - Temp'])

principalDf_ca = pd.DataFrame(data = principalComponents_ca
             , columns = ['principal component 1 - AOD', 'principal component 2 - AOD', 'principal component 3 - AOD', 'principal component 4 - AOD','principal component 5 - AOD'])

#add back in date column

principalDf_ct['date'] = pd.date_range(start='1/1/1986', periods = len(principalDf_ct), freq='D')
principalDf_ca['date'] = pd.date_range(start='1/1/1986', periods = len(principalDf_ca), freq='D')

#view explained variance

pca.explained_variance_ratio_

#view dfs

principalDf_ct
principalDf_ca

#separate train and test data

principalDf_ct['date'] = pd.to_datetime(principalDf_ct['date'])
principalDf_ca['date'] = pd.to_datetime(principalDf_ca['date'])

split_date = pd.datetime(1990,12,31)

ct_train = principalDf_ct.loc[principalDf_ct['date'] <= split_date]
ct_test = principalDf_ct.loc[principalDf_ct['date'] > split_date]

ca_train = principalDf_ca.loc[principalDf_ca['date'] <= split_date]
ca_test = principalDf_ca.loc[principalDf_ca['date'] > split_date]

#endregion



#region Create df for each PC 

#testing, training, and total sets

df1_test = ct_test['date'].copy()
temp1 = ct_test['principal component 1 - Temp'].copy()
aod1 = ca_test['principal component 1 - AOD'].copy()
df1_test = pd.concat([df1_test, temp1], axis = 1)
df1_test = pd.concat([df1_test, aod1], axis = 1)

df2_test = ct_test['date'].copy()
temp2 = ct_test[["principal component 2 - Temp"]].copy()
aod2 = ca_test[["principal component 2 - AOD"]].copy()
df2_test = pd.concat([df2_test, temp2], axis = 1)
df2_test = pd.concat([df2_test, aod2], axis = 1)

df3_test = ct_test['date'].copy()
temp3 = ct_test[["principal component 3 - Temp"]].copy()
aod3 = ca_test[["principal component 3 - AOD"]].copy()
df3_test = pd.concat([df3_test, temp3], axis = 1)
df3_test = pd.concat([df3_test, aod3], axis = 1)

df4_test = ct_test['date'].copy()
temp4 = ct_test[["principal component 4 - Temp"]] 
aod4 = ca_test[["principal component 4 - AOD"]].copy()
df4_test = pd.concat([df4_test, temp4], axis = 1)
df4_test = pd.concat([df4_test, aod4], axis = 1)

df5_test = ct_test['date'].copy()
temp5= ct_test[["principal component 5 - Temp"]].copy()
aod5 = ca_test[["principal component 5 - AOD"]].copy()
df5_test = pd.concat([df5_test, temp5], axis = 1)
df5_test = pd.concat([df5_test, aod5], axis = 1)

df1_train = ct_train['date'].copy()
temp1a = ct_train['principal component 1 - Temp'].copy()
aod1a = ca_train['principal component 1 - AOD'].copy()
df1_train = pd.concat([df1_train, temp1a], axis = 1)
df1_train = pd.concat([df1_train, aod1a], axis = 1)

df2_train = ct_train['date'].copy()
temp2a = ct_train[["principal component 2 - Temp"]].copy()
aod2a = ca_train[["principal component 2 - AOD"]].copy()
df2_train = pd.concat([df2_train, temp2a], axis = 1)
df2_train = pd.concat([df2_train, aod2a], axis = 1)

df3_train = ct_train['date'].copy()
temp3a = ct_train[["principal component 3 - Temp"]].copy()
aod3a = ca_train[["principal component 3 - AOD"]].copy()
df3_train = pd.concat([df3_train, temp3a], axis = 1)
df3_train = pd.concat([df3_train, aod3a], axis = 1)

df4_train = ct_train['date'].copy()
temp4a = ct_train[["principal component 4 - Temp"]].copy()
aod4a = ca_train[["principal component 4 - AOD"]].copy()
df4_train = pd.concat([df4_train, temp4a], axis = 1)
df4_train = pd.concat([df4_train, aod4a], axis = 1)

df5_train = ct_train['date'].copy()
temp5a = ct_train[["principal component 5 - Temp"]].copy()
aod5a = ca_train[["principal component 5 - AOD"]].copy()
df5_train = pd.concat([df5_train, temp5a], axis = 1)
df5_train = pd.concat([df5_train, aod5a], axis = 1)

df1_train.rename(columns = {'date':'ds'}, inplace = True)
df2_train.rename(columns = {'date':'ds'}, inplace = True)
df3_train.rename(columns = {'date':'ds'}, inplace = True)
df4_train.rename(columns = {'date':'ds'}, inplace = True)
df5_train.rename(columns = {'date':'ds'}, inplace = True)

df1_test.rename(columns = {'date':'ds'}, inplace = True)
df2_test.rename(columns = {'date':'ds'}, inplace = True)
df3_test.rename(columns = {'date':'ds'}, inplace = True)
df4_test.rename(columns = {'date':'ds'}, inplace = True)
df5_test.rename(columns = {'date':'ds'}, inplace = True)

df1_train.rename(columns = {'principal component 1 - Temp':'y'}, inplace = True)
df2_train.rename(columns = {'principal component 2 - Temp':'y'}, inplace = True)
df3_train.rename(columns = {'principal component 3 - Temp':'y'}, inplace = True)
df4_train.rename(columns = {'principal component 4 - Temp':'y'}, inplace = True)
df5_train.rename(columns = {'principal component 5 - Temp':'y'}, inplace = True)

df1_test.rename(columns = {'principal component 1 - Temp':'y'}, inplace = True)
df2_test.rename(columns = {'principal component 2 - Temp':'y'}, inplace = True)
df3_test.rename(columns = {'principal component 3 - Temp':'y'}, inplace = True)
df4_test.rename(columns = {'principal component 4 - Temp':'y'}, inplace = True)
df5_test.rename(columns = {'principal component 5 - Temp':'y'}, inplace = True)

df1_train.rename(columns = {'principal component 1 - AOD':'x'}, inplace = True)
df2_train.rename(columns = {'principal component 2 - AOD':'x'}, inplace = True)
df3_train.rename(columns = {'principal component 3 - AOD':'x'}, inplace = True)
df4_train.rename(columns = {'principal component 4 - AOD':'x'}, inplace = True)
df5_train.rename(columns = {'principal component 5 - AOD':'x'}, inplace = True)

df1_test.rename(columns = {'principal component 1 - AOD':'x'}, inplace = True)
df2_test.rename(columns = {'principal component 2 - AOD':'x'}, inplace = True)
df3_test.rename(columns = {'principal component 3 - AOD':'x'}, inplace = True)
df4_test.rename(columns = {'principal component 4 - AOD':'x'}, inplace = True)
df5_test.rename(columns = {'principal component 5 - AOD':'x'}, inplace = True)


df1 = df1_train
df1 = pd.concat([df1, df1_test], axis = 0)

df2 = df2_train
df2 = pd.concat([df2, df2_test], axis = 0)

df3 = df3_train
df3 = pd.concat([df3, df3_test], axis = 0)

df4 = df4_train
df4 = pd.concat([df4, df4_test], axis = 0)

df5 = df5_train
df5 = pd.concat([df5, df5_test], axis = 0)

#endregion



# region Neural Prophet

set_log_level("ERROR")

# Model and prediction

#PC1

m1 = NeuralProphet(
    growth = 'off',
    n_lags= 4,
    learning_rate=0.01,
    yearly_seasonality=False,
    weekly_seasonality=False,
    daily_seasonality=False,
)
m1.set_plotting_backend("matplotlib")

m1.add_lagged_regressor('x')

fig1a = df1.plot(x="ds", y=["y", "x"], figsize=(10, 6))

metrics1 = m1.fit(df1_train, freq = 'D', validation_df = df1_test, progress = None)
metrics1

test_metrics1 = m1.test(df1_test)
test_metrics1

fig, ax = plt.subplots(figsize=(20, 8))
ax.plot(metrics1["MAE"], '-o', label="Training Loss")  
ax.plot(metrics1["MAE_val"], '-r', label="Validation Loss")
ax.legend(loc='center right', fontsize=16)
ax.tick_params(axis='both', which='major', labelsize=20)
ax.set_xlabel("Epoch", fontsize=28)
ax.set_ylabel("Loss", fontsize=28)
ax.set_title("Model Loss (MAE) - PC1", fontsize=28)

forecast1 = m1.predict(df1)
m1.plot(forecast1)

m1.plot_components(forecast1, components=["lagged_regressors"])
m1.plot_parameters(components=["lagged_regressors"])
m1.plot_components(forecast1)
m1.plot_parameters(components=["autoregression"])
m1.plot_components(forecast1, components=["autoregression"])

df_residuals1 = pd.DataFrame({"ds": df1["ds"], "residuals": df1["y"] - forecast1["yhat1"]})
fig1 = df_residuals1.plot(x="ds", y="residuals", figsize=(10, 6))

#PC2

m2 = NeuralProphet(
    growth = 'off',
    n_lags= 4,
    learning_rate=0.01,
    yearly_seasonality=False,
    weekly_seasonality=False,
    daily_seasonality=False,
)
m2.set_plotting_backend("matplotlib")

m2.add_lagged_regressor('x')

fig2a = df2.plot(x="ds", y=["y", "x"], figsize=(10, 6))

metrics2 = m2.fit(df2_train, freq = 'D',  validation_df = df2_test, progress = None)
metrics2

test_metrics2 = m2.test(df2_test)
test_metrics2

fig, ax = plt.subplots(figsize=(20, 8))
ax.plot(metrics2["MAE"], '-o', label="Training Loss")  
ax.plot(metrics2["MAE_val"], '-r', label="Validation Loss")
ax.legend(loc='center right', fontsize=16)
ax.tick_params(axis='both', which='major', labelsize=20)
ax.set_xlabel("Epoch", fontsize=28)
ax.set_ylabel("Loss", fontsize=28)
ax.set_title("Model Loss (MAE) - PC2", fontsize=28)

forecast2 = m2.predict(df2)
m2.plot(forecast2)

m2.plot_components(forecast2, components=["lagged_regressors"])
m2.plot_parameters(components=["lagged_regressors"])
m2.plot_components(forecast2)
m2.plot_parameters(components=["autoregression"])
m2.plot_components(forecast2, components=["autoregression"])

df_residuals2 = pd.DataFrame({"ds": df2["ds"], "residuals": df2["y"] - forecast2["yhat1"]})
fig2 = df_residuals2.plot(x="ds", y="residuals", figsize=(10, 6))



#PC3

m3 = NeuralProphet(
    growth = 'off',
    n_lags= 4,
    learning_rate=0.01,
    yearly_seasonality=False,
    weekly_seasonality=False,
    daily_seasonality=False,
)
m3.set_plotting_backend("matplotlib")

m3.add_lagged_regressor('x')

fig3a = df3.plot(x="ds", y=["y", "x"], figsize=(10, 6))

metrics3 = m3.fit(df3_train, freq = 'D',  validation_df = df3_test, progress = None)
metrics3

test_metrics3 = m3.test(df3_test)
test_metrics3

fig, ax = plt.subplots(figsize=(20, 8))
ax.plot(metrics3["MAE"], '-o', label="Training Loss")  
ax.plot(metrics3["MAE_val"], '-r', label="Validation Loss")
ax.legend(loc='center right', fontsize=16)
ax.tick_params(axis='both', which='major', labelsize=20)
ax.set_xlabel("Epoch", fontsize=28)
ax.set_ylabel("Loss", fontsize=28)
ax.set_title("Model Loss (MAE) - PC3", fontsize=28)

forecast3 = m3.predict(df3)
m3.plot(forecast3)

m3.plot_components(forecast3, components=["lagged_regressors"])
m3.plot_parameters(components=["lagged_regressors"])
m3.plot_components(forecast3)
m3.plot_parameters(components=["autoregression"])
m3.plot_components(forecast3, components=["autoregression"])

df_residuals3 = pd.DataFrame({"ds": df3["ds"], "residuals": df3["y"] - forecast3["yhat1"]})
fig3 = df_residuals3.plot(x="ds", y="residuals", figsize=(10, 6))


#PC4

m4 = NeuralProphet(
    growth = 'off',
    n_lags= 4,
    learning_rate=0.01,
    yearly_seasonality=False,
    weekly_seasonality=False,
    daily_seasonality=False,
)
m4.set_plotting_backend("matplotlib")

m4.add_lagged_regressor('x')

fig4a = df4.plot(x="ds", y=["y", "x"], figsize=(10, 6))

metrics4 = m4.fit(df4_train, freq = 'D',  validation_df = df4_test, progress = None)
metrics4

test_metrics4 = m4.test(df4_test)
test_metrics4

fig, ax = plt.subplots(figsize=(20, 8))
ax.plot(metrics4["MAE"], '-o', label="Training Loss")  
ax.plot(metrics4["MAE_val"], '-r', label="Validation Loss")
ax.legend(loc='center right', fontsize=16)
ax.tick_params(axis='both', which='major', labelsize=20)
ax.set_xlabel("Epoch", fontsize=28)
ax.set_ylabel("Loss", fontsize=28)
ax.set_title("Model Loss (MAE) - PC4", fontsize=28)

forecast4 = m4.predict(df4)
m4.plot(forecast4)

m4.plot_components(forecast4, components=["lagged_regressors"])
m4.plot_parameters(components=["lagged_regressors"])
m4.plot_components(forecast4)
m4.plot_parameters(components=["autoregression"])
m4.plot_components(forecast4, components=["autoregression"])

df_residuals4 = pd.DataFrame({"ds": df4["ds"], "residuals": df5["y"] - forecast4["yhat1"]})
fig4 = df_residuals4.plot(x="ds", y="residuals", figsize=(10, 6))

m5 = NeuralProphet(
    growth = 'off',
    n_lags= 4,
    learning_rate=0.01,
    yearly_seasonality=False,
    weekly_seasonality=False,
    daily_seasonality=False,
)
m5.set_plotting_backend("matplotlib")

m5.add_lagged_regressor('x')

fig5a = df5.plot(x="ds", y=["y", "x"], figsize=(10, 6))

metrics5 = m5.fit(df5_train, freq = 'D',  validation_df = df5_test, progress = None)
metrics5

test_metrics5 = m5.test(df5_test)
test_metrics5

fig, ax = plt.subplots(figsize=(20, 8))
ax.plot(metrics5["MAE"], '-o', label="Training Loss")  
ax.plot(metrics5["MAE_val"], '-r', label="Validation Loss")
ax.legend(loc='center right', fontsize=16)
ax.tick_params(axis='both', which='major', labelsize=20)
ax.set_xlabel("Epoch", fontsize=28)
ax.set_ylabel("Loss", fontsize=28)
ax.set_title("Model Loss (MAE) - PC5", fontsize=28)

forecast5 = m5.predict(df5)
m5.plot(forecast5)

m5.plot_components(forecast5, components=["lagged_regressors"])
m5.plot_parameters(components=["lagged_regressors"])
m5.plot_components(forecast5)
m5.plot_parameters(components=["autoregression"])
m5.plot_components(forecast5, components=["autoregression"])

df_residuals5 = pd.DataFrame({"ds": df5["ds"], "residuals": df5["y"] - forecast5["yhat1"]})
fig5 = df_residuals5.plot(x="ds", y="residuals", figsize=(10, 6))



#endregion



# region Backscale

#make new dataframe with predicted values (values after split date)

yhat1 = forecast1.loc[forecast1['ds'] > split_date]
df1_predict = df1_train['y'].copy()
yhat1 = yhat1['yhat1'].copy()
df1_predict = pd.concat([df1_predict, yhat1], axis = 0)

yhat2 = forecast2.loc[forecast2['ds'] > split_date]
df2_predict = df2_train['y'].copy()
yhat2 = yhat2['yhat1'].copy()
df2_predict = pd.concat([df2_predict, yhat2], axis = 0)

yhat3 = forecast3.loc[forecast3['ds'] > split_date]
df3_predict = df3_train['y'].copy()
yhat3 = yhat3['yhat1'].copy()
df3_predict = pd.concat([df3_predict, yhat3], axis = 0) 

yhat4 = forecast4.loc[forecast4['ds'] > split_date]
df4_predict = df4_train['y'].copy()
yhat4 = yhat4['yhat1'].copy()
df4_predict = pd.concat([df4_predict, yhat4], axis = 0)

yhat5 = forecast5.loc[forecast5['ds'] > split_date]
df5_predict = df5_train['y'].copy()
yhat5 = yhat5['yhat1'].copy()
df5_predict = pd.concat([df5_predict, yhat5], axis = 0)

#combine all predicted dataframes for each PC

df_predict = pd.concat([df1_predict, df2_predict, df3_predict, df4_predict, df5_predict], axis = 1)

#recomp with inverse transform

ct_orig = np.dot(df_predict,pca.components_)
ct_orig_backscaled = scaler_ct.inverse_transform(ct_orig)

ct_orig = pd.DataFrame(ct_orig)
ct_orig_backscaled = pd.DataFrame(ct_orig_backscaled)

ct_orig_backscaled['date'] = pd.date_range(start='1/1/1986', periods = len(principalDf_ct), freq='D')

ct_orig_backscaled = ct_orig_backscaled.set_index('date')
ct_orig_backscaled.columns = clm_t.columns

df_ca = principalDf_ca
df_ca = df_ca.drop(['date'], axis = 1)

ca_orig = np.dot(df_ca,pca.components_)
ca_orig_backscaled = scaler_ca.inverse_transform(ca_orig)

ca_orig = pd.DataFrame(ca_orig)
ca_orig_backscaled = pd.DataFrame(ca_orig_backscaled)

ca_orig_backscaled['date'] = pd.date_range(start='1/1/1986', periods = len(principalDf_ca), freq='D')

ca_orig_backscaled = ca_orig_backscaled.set_index('date')
ca_orig_backscaled.columns = clm_a.columns

#non-predicted values back-scaled

df_ct = principalDf_ct
df_ct = df_ct.drop(['date'], axis = 1)

ct_np = np.dot(df_ct,pca.components_)
ct_np_backscaled = scaler_ct.inverse_transform(ct_np)

ct_np = pd.DataFrame(ct_np)
ct_np_backscaled = pd.DataFrame(ct_np_backscaled)

ct_np_backscaled['date'] = pd.date_range(start='1/1/1986', periods = len(principalDf_ca), freq='D')

ct_np_backscaled = ct_np_backscaled.set_index('date')
ct_np_backscaled.columns = clm_t.columns

#export to R for data processing

filepath5 = Path('/Users/rachelalaynahall/Desktop/ct_bs.csv')  
filepath5.parent.mkdir(parents=True, exist_ok=True)  
ct_orig_backscaled.to_csv(filepath5)

filepath6 = Path('/Users/rachelalaynahall/Desktop/ca_bs.csv')  
filepath6.parent.mkdir(parents=True, exist_ok=True)  
ca_orig_backscaled.to_csv(filepath6)

filepath7 = Path('/Users/rachelalaynahall/Desktop/ct_np.csv')  
filepath7.parent.mkdir(parents=True, exist_ok=True)  
ct_np_backscaled.to_csv(filepath7)

### R CODE BLOCK ###

```{r}
ct_bs = read.csv("~/Desktop/ct_bs.csv")

```


```{r}
rownames(ct_bs)<-unlist(ct_bs[,1])
ct_bs = ct_bs[-c(1)]

ct_bs = ct_bs %>% rownames_to_column('row') %>% pivot_longer(cols = -row)

ct_bs = ct_bs %>% 
  rename(
    date = row,
    location = name,
    T = value
    )
```





```{r}
write.csv(ct_bs, "~/Desktop/c_ct_bs.csv", row.names = FALSE)
```



```{r}
ca_bs = read.csv("~/Desktop/ca_bs.csv")

```


```{r}
rownames(ca_bs)<-unlist(ca_bs[,1])
ca_bs = ca_bs[-c(1)]

ca_bs = ca_bs %>% rownames_to_column('row') %>% pivot_longer(cols = -row)

ca_bs = ca_bs %>% 
  rename(
    date = row,
    location = name,
    A = value
    )
```


```{r}
write.csv(ca_bs, "~/Desktop/c_ca_bs.csv", row.names = FALSE)
```


```{r}
ct_np = read.csv("~/Desktop/ct_np.csv")

```


```{r}
rownames(ct_np)<-unlist(ct_np[,1])
ct_np = ct_np[-c(1)]

ct_np = ct_np %>% rownames_to_column('row') %>% pivot_longer(cols = -row)

ct_np = ct_np %>% 
  rename(
    date = row,
    location = name,
    T = value
    )
```


```{r}
write.csv(ct_np, "~/Desktop/c_ct_np.csv", row.names = FALSE)
```
### END R CODE BLOCK ###

#import from R

#temperature

ct_bs = pd.read_csv('/Users/rachelalaynahall/Desktop/c_ct_bs.csv')

#split location back into latitude and longitude by separator ".."

ct_bs[['lat', 'lon']] = ct_bs["location"].apply(lambda x: pd.Series(str(x).split("..")))

#subtract 90 and 180 from latitude and longitude respectively to get actual locations back

ct_bs['lat'] = ct_bs['lat'].str.replace('X', '')
ct_bs["lat"] = ct_bs["lat"].astype(float)
ct_bs["lon"] = ct_bs["lon"].astype(float)
ct_bs["lat"] = ct_bs["lat"] - 90
ct_bs["lon"] = ct_bs["lon"] - 180
ct_bs = ct_bs.drop(['location'], axis = 1)
ct_bs['date'] = pd.to_datetime(ct_bs['date'])
ct_bs.rename(columns = {'date':'time'}, inplace = True)

#add climatologies back in

ct_bs_array = ct_bs.set_index(["time", "lat", "lon"]).to_xarray()
dtb_month = ct_bs_array.groupby('time.month') + ct_month

tf = dtb_month.to_dataframe()
tf = tf.drop(['month'], axis = 1)

#make dataframe to compare recomped and predicted values to

compare = dt.to_dataframe()

#import from R

#AOD

ca_bs = pd.read_csv('/Users/rachelalaynahall/Desktop/c_ca_bs.csv')

#split location back into latitude and longitude by separator ".."

ca_bs[['lat', 'lon']] = ca_bs["location"].apply(lambda x: pd.Series(str(x).split("..")))

ca_bs['lat'] = ca_bs['lat'].str.replace('X', '')
ca_bs["lat"] = ca_bs["lat"].astype(float)
ca_bs["lon"] = ca_bs["lon"].astype(float)

#subtract 90 and 180 from latitude and longitude respectively to get actual locations back

ca_bs["lat"] = ca_bs["lat"] - 90
ca_bs["lon"] = ca_bs["lon"] - 180
ca_bs = ca_bs.drop(['location'], axis = 1)
ca_bs['date'] = pd.to_datetime(ca_bs['date'])
ca_bs.rename(columns = {'date':'time'}, inplace = True)

#add climatologies back in

ca_bs_array = ca_bs.set_index(["time", "lat", "lon"]).to_xarray()
dab_month = ca_bs_array.groupby('time.month') + ca_month

af = dab_month.to_dataframe()
af = af.drop(['month'], axis = 1)

#make dataframe to compare recomped and predicted values to

compare_a = da.to_dataframe()

#import from R

#non-predicted temperature

ct_np = pd.read_csv('/Users/rachelalaynahall/Desktop/c_ct_np.csv')

#split location back into latitude and longitude by separator ".."

ct_np[['lat', 'lon']] = ct_np["location"].apply(lambda x: pd.Series(str(x).split("..")))

ct_np['lat'] = ct_np['lat'].str.replace('X', '')
ct_np["lat"] = ct_np["lat"].astype(float)
ct_np["lon"] = ct_np["lon"].astype(float)

#subtract 90 and 180 from latitude and longitude respectively to get actual locations back

ct_np["lat"] = ct_np["lat"] - 90
ct_np["lon"] = ct_np["lon"] - 180
ct_np = ct_np.drop(['location'], axis = 1)
ct_np['date'] = pd.to_datetime(ct_np['date'])
ct_np.rename(columns = {'date':'time'}, inplace = True)

#add climatologies back in

ct_np_array = ct_np.set_index(["time", "lat", "lon"]).to_xarray()
dtn_month = ct_np_array.groupby('time.month') + ct_month

tn = dtn_month.to_dataframe()
tn = tn.drop(['month'], axis = 1)

#make dataframe to compare recomped and predicted values to

compare_t = da.to_dataframe()

#export to R for data processing

filepath8 = Path('/Users/rachelalaynahall/Desktop/tf.csv')  
filepath8.parent.mkdir(parents=True, exist_ok=True)  
tf.to_csv(filepath8)

filepath9 = Path('/Users/rachelalaynahall/Desktop/compare.csv')  
filepath9.parent.mkdir(parents=True, exist_ok=True)  
compare.to_csv(filepath9)

filepath10 = Path('/Users/rachelalaynahall/Desktop/af.csv')  
filepath10.parent.mkdir(parents=True, exist_ok=True)  
af.to_csv(filepath10)

filepath11 = Path('/Users/rachelalaynahall/Desktop/compare_a.csv')  
filepath11.parent.mkdir(parents=True, exist_ok=True)  
compare_a.to_csv(filepath11)

filepath12 = Path('/Users/rachelalaynahall/Desktop/tn.csv')  
filepath12.parent.mkdir(parents=True, exist_ok=True)  
tn.to_csv(filepath12)

filepath13 = Path('/Users/rachelalaynahall/Desktop/compare_t.csv')  
filepath13.parent.mkdir(parents=True, exist_ok=True)  
compare_t.to_csv(filepath13)

#endregion

### R CODE BLOCK ###


```{r}
tf = read.csv("~/Desktop/tf.csv")
compare = read.csv("~/Desktop/compare.csv")

difference = tf['time']
difference['lat'] = tf['lat']
difference['lon'] = tf['lon']
difference['diff'] = compare['T'] - tf['T']

difference$location = str_c(difference$lat, ', ', difference$lon)

difference['diff2'] = difference['diff']^2

diff_loc = difference %>%
  group_by(lat,lon) %>%
  summarise(mean_diff = mean(diff))

min = min(diff_loc$mean_diff)
min

max = max(diff_loc$mean_diff)
max

range = max - min
range

mean_loc = mean(diff_loc$mean_diff)
mean_loc

diff_loc$mean_diff = diff_loc$mean_diff^2
diff_loc$mean_diff = rescale(diff_loc$mean_diff, to = c(0, 10))

ggplot(diff_loc, aes(x = lon, y = lat)) +
  geom_tile(aes(fill = mean_diff), colour = "white") +
  scale_fill_gradient(low = "white", high = "black") +
  theme_minimal() + 
  ggtitle("Temperature Mean Diff^2 by Location (Predicted Data)")

```

```{r}
ggplot(data=difference %>% mutate(time=as_date(time)) %>% group_by(time) %>% summarize(m=mean(diff^2)),aes(x=time,y=m)) + geom_line() + 
  ggtitle("Temperature Mean Diff^2 by Time (Predicted Data)")

diff_time = difference %>%
  group_by(time) %>%
  summarise(mean_diff = mean(diff))

min_time = min(diff_time$mean_diff)
min_time

max_time = max(diff_time$mean_diff)
max_time

range_time = max_time - min_time
range_time

mean_time = mean(diff_time$mean_diff)
mean_time

```

```{r}
af = read.csv("~/Desktop/af.csv")
compare_a = read.csv("~/Desktop/compare_a.csv")

difference_a = af['time']
difference_a['lat'] = af['lat']
difference_a['lon'] = af['lon']
difference_a['diff'] = compare_a['A'] - af['A']

difference_a$location = str_c(difference_a$lat, ', ', difference_a$lon)

difference_a['diff2'] = difference_a['diff']^2

diff_loc_a = difference_a %>%
  group_by(lat,lon) %>%
  summarise(mean_diff = mean(diff))

mina = min(diff_loc_a$mean_diff)
mina

maxa = max(diff_loc_a$mean_diff)
maxa

diff_loc_a$mean_diff = diff_loc_a$mean_diff^2
diff_loc_a$mean_diff = rescale(diff_loc_a$mean_diff, to = c(0, 10))

ggplot(diff_loc_a, aes(x = lon, y = lat)) +
  geom_tile(aes(fill = mean_diff), colour = "white") +
  scale_fill_gradient(low = "white", high = "black") +
  theme_minimal() + 
  ggtitle("AOD Mean Diff^2 by Location")

```


```{r}
ggplot(data=difference_a %>% mutate(time=as_date(time)) %>% group_by(time) %>% summarize(m=mean(diff^2)),aes(x=time,y=m)) + geom_line() + 
  ggtitle("AOD Mean Diff^2 by Time")

```
```{r}
tn = read.csv("~/Desktop/tn.csv")

difference_t = tn['time']
difference_t['lat'] = tn['lat']
difference_t['lon'] = tn['lon']
difference_t['diff'] = compare['T'] - tn['T']

difference_t$location = str_c(difference_t$lat, ', ', difference_t$lon)

difference_t['diff2'] = difference_t['diff']^2

diff_loc_t = difference_t %>%
  group_by(lat,lon) %>%
  summarise(mean_diff = mean(diff))

mint = min(diff_loc_t$mean_diff)
mint

maxt = max(diff_loc_t$mean_diff)
maxt

diff_loc_t$mean_diff = diff_loc_t$mean_diff^2
diff_loc_t$mean_diff = rescale(diff_loc_t$mean_diff, to = c(0, 10))

ggplot(diff_loc_t, aes(x = lon, y = lat)) +
  geom_tile(aes(fill = mean_diff), colour = "white") +
  scale_fill_gradient(low = "white", high = "black") +
  theme_minimal() + 
  ggtitle("Temperature Mean Diff^2 by Location")

```
```{r}
ggplot(data=difference_t %>% mutate(time=as_date(time)) %>% group_by(time) %>% summarize(m=mean(diff^2)),aes(x=time,y=m)) + geom_line() + 
  ggtitle("Temperature Mean Diff^2 by Time")
```


```{r}
write.csv(difference, "~/Desktop/difference.csv", row.names = FALSE)

write.csv(difference_a, "~/Desktop/difference_a.csv", row.names = FALSE)

write.csv(difference_t, "~/Desktop/difference_t.csv", row.names = FALSE)
```

### END R CODE BLOCK ###

#region Plots

temperature = dtb_month['T']
anomaly = ct_bs_array['T']
aod = dab_month['A']
aod_anomaly = ca_bs_array['A']

dtb_month.hvplot(groupby = 'time', cmap = "viridis")
ct_bs_array.hvplot(groupby = 'time', cmap = "fire")

dtb_month.hvplot(
    groupby="time",  # adds a widget for time
    clim=(210, 274),  # sets colormap limits
    widget_type="scrubber",
    widget_location="bottom", 
    cmap = "coolwarm"
)

ct_bs_array.hvplot(
    groupby="time",  # adds a widget for time
    clim=(-15, 15),  # sets colormap limits
    widget_type="scrubber",
    widget_location="bottom",
    cmap = "seismic"
)


dab_month.hvplot(
    groupby="time",  # adds a widget for time
    clim=(-0.0485,0.3203),  # sets colormap limits
    widget_type="scrubber",
    widget_location="bottom", 
    cmap = "PRGn"
)

ca_bs_array.hvplot(
    groupby="time",  # adds a widget for time
    clim=(-0.1, .4),  # sets colormap limits
    widget_type="scrubber",
    widget_location="bottom",
    cmap = "BuPu"
)

nbr = 4727

for j in range(nbr):
    cbar_kwargs = {
        'orientation':'horizontal',
        'fraction': 0.045,
        'pad': 0.01,
        'extend':'neither'
    }

    fig = plt.figure(figsize=(20,20))
    ax = fig.add_subplot(1,1,1, projection = ccrs.PlateCarree())
    ax.add_feature(NaturalEarthFeature('cultural', 'admin_0_countries', '10m'),
                        facecolor='none', edgecolor='black')
    ax.set_extent([-150, 150, -55, 85])

    date =  pd.to_datetime(anomaly.isel(time=j )['time'].values)
    ax.set_title("Temperature Anomalies on " + str(date.month) + "/" + str(date.day) + "/" + str(date.year))
    anomaly.isel(time=j ).plot.imshow(ax=ax, add_labels=False, add_colorbar=True,
                vmin=-5, vmax=5, cmap='coolwarm',
                cbar_kwargs=cbar_kwargs, interpolation='bicubic')
    plt.savefig("2_global_map" + str(j ) + ".png", bbox_inches='tight', dpi=150)

for j  in range(nbr):
    cbar_kwargs = {
        'orientation':'horizontal',
        'fraction': 0.045,
        'pad': 0.01,
        'extend':'neither'
    }

    fig = plt.figure(figsize=(20,20))
    ax = fig.add_subplot(1,1,1, projection = ccrs.PlateCarree())
    ax.add_feature(NaturalEarthFeature('cultural', 'admin_0_countries', '10m'),
                        facecolor='none', edgecolor='black')
    ax.set_extent([-150, 150, -55, 85])

    date =  pd.to_datetime(temperature.isel(time=j )['time'].values)
    ax.set_title("Temperature on " + str(date.month) + "/" + str(date.day) + "/" + str(date.year))
    temperature.isel(time=j ).plot.imshow(ax=ax, add_labels=False, add_colorbar=True,
                vmin=241.5, vmax=263.5, cmap='coolwarm',
                cbar_kwargs=cbar_kwargs, interpolation='bicubic')
    plt.savefig("2_temp_global_map" + str(j ) + ".png", bbox_inches='tight', dpi=150)

for j  in range(nbr):
    cbar_kwargs = {
        'orientation':'horizontal',
        'fraction': 0.045,
        'pad': 0.01,
        'extend':'neither'
    }

    fig = plt.figure(figsize=(20,20))
    ax = fig.add_subplot(1,1,1, projection = ccrs.PlateCarree())
    ax.add_feature(NaturalEarthFeature('cultural', 'admin_0_countries', '10m'),
                        facecolor='none', edgecolor='black')
    ax.set_extent([-150, 150, -55, 85])

    date =  pd.to_datetime(aod_anomaly.isel(time=j )['time'].values)
    ax.set_title("AOD Anomalies on " + str(date.month) + "/" + str(date.day) + "/" + str(date.year))
    aod_anomaly.isel(time=j ).plot.imshow(ax=ax, add_labels=False, add_colorbar=True,
                vmin=-0.1, vmax=0.2, 
                cmap='BuPu',
                cbar_kwargs=cbar_kwargs, interpolation='bicubic')
    plt.savefig("2_anom_aod_global_map" + str(j ) + ".png", bbox_inches='tight', dpi=150)

for j  in range(nbr):
    cbar_kwargs = {
        'orientation':'horizontal',
        'fraction': 0.045,
        'pad': 0.01,
        'extend':'neither'
    }

    fig = plt.figure(figsize=(20,20))
    ax = fig.add_subplot(1,1,1, projection = ccrs.PlateCarree())
    ax.add_feature(NaturalEarthFeature('cultural', 'admin_0_countries', '10m'),
                        facecolor='none', edgecolor='black')
    ax.set_extent([-150, 150, -55, 85])

    date =  pd.to_datetime(aod.isel(time=j )['time'].values)
    ax.set_title("AOD on " + str(date.month) + "/" + str(date.day) + "/" + str(date.year))
    aod.isel(time=j ).plot.imshow(ax=ax, add_labels=False, add_colorbar=True,
                vmin=-0.1, vmax=0.4, cmap='BuPu',
                cbar_kwargs=cbar_kwargs, interpolation='bicubic')
    plt.savefig("2_aod_global_map" + str(j ) + ".png", bbox_inches='tight', dpi=150)

#endregion



n_pcs= pca.n_components_ # get number of component
# get the index of the most important feature on EACH component
most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]
initial_feature_names = clm_t.columns
# get the most important feature names
most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]
most_important_names
